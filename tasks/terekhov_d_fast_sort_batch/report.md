# Быстрая сортировка с четно-нечетным слиянием Бэтчера
- **Студент**: Терехов Дмитрий Юрьевич 
- **группа:** 3823Б1ПР4
- **Технология**: SEQ | MPI
- **Вариант**: 15 (Быстрая сортировка с четно-нечетным слиянием Бэтчера.)

## 1. Введение
Сортировка является одной из фундаментальных операций в информатике, используемой в широком спектре приложений от баз данных до научных вычислений. Быстрая сортировка (Quicksort) - один из наиболее эффективных алгоритмов сортировки со средней сложностью O(n log n). Четно-нечетное слияние Бэтчера (Batcher's odd-even merge) - это параллельный алгоритм слияния, который особенно эффективен в параллельных системах благодаря своей регулярной структуре сравнений и обменов.

## 2. Постановка задачи

**Цель работы:**
Реализовать и сравнить производительность последовательного (SEQ) и параллельного (MPI) алгоритмов быстрой сортировки с четно-нечетным слиянием Бэтчера, где массив распределяется между процессами, каждый процесс выполняет локальную сортировку, а затем применяется алгоритм Бэтчера для глобального слияния.

**Определение задачи:**

Для заданного массива целых чисел размером N необходимо отсортировать его в порядке возрастания, используя:

- Последовательный алгоритм быстрой сортировки с четно-нечетным слиянием

- Параллельный алгоритм с распределением данных между процессами MPI

**Ограничения:**

- Входные данные - вектор целых чисел произвольного размера

- Для параллельной реализации используется MPI с алгоритмом четно-нечетного слияния Бэтчера

- Данные распределяются примерно равными частями между процессами

- Результат обеих реализаций (последовательной и параллельной) должен совпадать

## 3. Базовый алгоритм

### Алгоритм последовательной реализации

1. Инициализация

- Получить на вход вектор данных

- Инициализировать результирующий вектор

2. Сортировка массива

- Использование оптимизированного алгоритма сортировки

**Код последовательной реализации (ключевая часть):**

```cpp
bool TerekhovDFastSortBatchSEQ::RunImpl() {
  const auto &input_data = GetInput();
  if (input_data.empty()) {
    return true;
  }

  std::vector<int> sorted_buffer = input_data;
  std::sort(sorted_buffer.begin(), sorted_buffer.end());

  GetOutput() = std::move(sorted_buffer);
  return true;
}
```

## 4. Схема распараллеливания
### 4.1. Распределение данных
**Для параллельной обработки используется алгоритм Бэтчера:**

1. Распределение массива:

- Массив разбивается на сегменты по процессам

- Каждый процесс получает примерно равный сегмент

- Для N элементов и P процессов: базовый размер = N/P, остаток распределяется первым процессам

2. Локальная сортировка:

- Каждый процесс сортирует свой сегмент быстрой сортировкой

3. Глобальное слияние Бэтчера:

- Применяются четные и нечетные фазы обмена

- В четных фазах обмениваются процессы с четными номерами

- В нечетных фазах обмениваются процессы с нечетными номерами

### 4.2. Схема связи и топология
**Используется звездообразная топология с процессом 0 в качестве координатора:**
**Нисходящие связи (от процесса 0 к worker-процессам):**

- Рассылка размеров массива `(MPI_Bcast)`

- Рассылка всего массива `(MPI_Bcast)`

- Распределение границ сегментов `(MPI_Send)`

**Восходящие связи (от worker-процессов к процессу 0):**

- Передача отсортированных сегментов `(MPI_Send)`

**Горизонтальные связи (между процессами для алгоритма Бэтчера):**

- Обмен данными между соседними процессами `(MPI_Sendrecv)`


### 4.3. Ранжирование ролей    
**Процесс 0 (Master-координатор):**

1. Инициализация и рассылка данных

2. Распределение сегментов между процессами

3. Локальная сортировка своего сегмента

4. Координация фаз алгоритма Бэтчера

5. Сбор результатов от всех процессов

6. Рассылка финального результата

**Процессы 1..N-1 (Worker-процессы):**

1. Получение данных и границ сегментов

2. Локальная сортировка своего сегмента

3. Участие в фазах алгоритма Бэтчера

4. Отправка результатов процессу 0

5. Получение финального результата

### 4.4. Псевдокод алгоритма Бэтчера
```cpp
// Основная функция построения сети сортировки
function ConstructSortNetwork(processes):
    stack = empty stack
    stack.push((processes, false))  // (список процессов, флаг фазы слияния)
    
    while stack not empty:
        (current_processes, merge_phase) = stack.pop()
        
        if size(current_processes) <= 1:
            continue
        
        if not merge_phase:
            // Фаза разделения (divide)
            mid = size(current_processes) / 2
            left = processes[0:mid]
            right = processes[mid:end]
            
            stack.push((current_processes, true))   // Следующая фаза - слияние
            stack.push((right, false))              // Рекурсивно для правой части
            stack.push((left, false))               // Рекурсивно для левой части
        else:
            // Фаза слияния (merge)
            mid = size(current_processes) / 2
            upper = processes[0:mid]     // Верхняя половина
            lower = processes[mid:end]   // Нижняя половина
            ConstructMergeNetwork(upper, lower)

// Функция построения сети слияния
function ConstructMergeNetwork(upper, lower):
    stack = empty stack
    stack.push((upper, lower, false))  // (верхние процессы, нижние процессы, флаг)
    
    while stack not empty:
        (upper_part, lower_part, merge_phase) = stack.pop()
        total_size = size(upper_part) + size(lower_part)
        
        if total_size <= 1:
            continue
        
        if total_size == 2:
            // Базовый случай: один компаратор
            comparator_pairs.add((upper_part[0], lower_part[0]))
            continue
        
        if not merge_phase:
            // Разделение на четные и нечетные элементы
            (upper_odd, upper_even) = SeparateOddEven(upper_part)
            (lower_odd, lower_even) = SeparateOddEven(lower_part)
            
            stack.push((upper_part, lower_part, true))  // Следующая фаза
            stack.push((upper_even, lower_even, false)) // Рекурсивно для четных
            stack.push((upper_odd, lower_odd, false))   // Рекурсивно для нечетных
        else:
            // Фаза построения компараторов
            merged = concatenate(upper_part, lower_part)
            for i = 1 to size(merged)-2 step 2:
                comparator_pairs.add((merged[i], merged[i+1]))

// Вспомогательная функция разделения на четные и нечетные
function SeparateOddEven(elements):
    odd = []
    even = []
    for i = 0 to size(elements)-1:
        if i % 2 == 0:
            even.add(elements[i])
        else:
            odd.add(elements[i])
    return (odd, even)

// Выполнение сети компараторов
function ExecuteComparatorPairs(comparator_pairs, local_data):
    для каждой пары (process_a, process_b) в comparator_pairs:
        если текущий процесс == process_a или текущий процесс == process_b:
            partner = (current == process_a) ? process_b : process_a
            // Обмен данными с партнером
            send_data_to(partner, local_data)
            receive_data_from(partner, partner_data)
            
            // Слияние и выбор элементов
            if current == process_a:
                // Процесс A получает меньшие элементы
                local_data = merge_and_select_smaller(local_data, partner_data)
            else:
                // Процесс B получает большие элементы
                local_data = merge_and_select_larger(local_data, partner_data)
```
## 5. Детали реализации
### Структура кода
**Файловая структура:**
```cpp
terekhov_d_fast_sort_batch/
├── common/include/common.hpp
├── seq/include/ops_seq.hpp
├── seq/src/ops_seq.cpp
├── mpi/include/ops_mpi.hpp
├── mpi/src/ops_mpi.cpp
├── tests/functional/main.cpp
├── tests/performance/main.cpp
└── settings.json
```
**Ключевые классы:**

**1. Последовательная реализация (seq):**

- `TerekhovDFastSortBatchSEQ` - класс последовательной реализации

- Основной метод: `RunImpl()` - использует `std::sort`

**2. MPI реализация (`mpi`):**

- `TerekhovDFastSortBatchMPI` - класс параллельной реализации

- **Основные методы:**

- - `RunImpl()` - использует std::sort из стандартной библиотеки C++"

- - `BroadcastInputData()` - рассылка входных данных

- - `GetProcessSegment()` - вычисление границ сегментов

- - `SortLocalData()` - локальная быстрая сортировка

- - `BatcherMergeProcedure()` - алгоритм Бэтчера

- - `BatcherEvenStep()` & `BatcherOddStep()`- четные/нечетные фазы

- - `ProcessExchange()` -  обмен данными между процессами

- - `CombineAndSelect()` - слияние и выбор части данных

**Архитектурные особенности:**

- Использование MPI для межпроцессного взаимодействия
- Алгоритм четно-нечетного слияния Бэтчера для глобального слияния
- Централизованная модель с процессом-координатором (rank 0)

### Важные допущения и ограничения
**Обработка данных:**

- Поддержка массивов произвольного размера
- Использование типа int для элементов
- Проверка корректности в `ValidationImpl()`

**Граничные случаи:**

- Пустой массив
- Массивы из одного элемента
- Уже отсортированные массивы
- Очень большие массивы (75 миллионов элементов в тестах производительности)

## 6. Экспериментальная установка
### Аппаратное обеспечение и ОС
**Системные характеристики:**

- Модель процессора: intel core i5 10400f
- Архитектура: x86_64
- Ядра/потоки: 6 ядер 12 потоков
- Оперативная память: 32 GB 
- Операционная система: Windows 10
- Тип системы: PC (personal computer)

### Набор инструментов
**Компиляция и сборка:**

- Компиляторы: GCC и Clang
- Среда разработки: Visual Studio Code
- Стандарт языка: C++17
- Система сборки: CMake
- Тип сборки: Release

### Управление процессами
Запуск тестов:
```cpp
bash

# Функциональные тесты на 4 процессах
mpirun --allow-run-as-root -n 4 ./build/bin/ppc_func_tests

# Тесты производительности на 4 процессах  
mpirun --allow-run-as-root -n 4 ./build/bin/ppc_perf_tests

# Тесты на 2 процессах
mpirun --allow-run-as-root -n 2 ./build/bin/ppc_func_tests
mpirun --allow-run-as-root -n 2 ./build/bin/ppc_perf_tests

# Тесты на 1 процессе
mpirun --allow-run-as-root -n 1 ./build/bin/ppc_func_tests
mpirun --allow-run-as-root -n 1 ./build/bin/ppc_perf_tests
```

## 7. Результаты и обсуждение
### 7.1 Корректность
**Методы проверки корректности:**

1. Функциональное тестирование: 26 тестов в наборе `TerekhovDFastSortBatchFuncTests`

2. Тестирование производительности: 4 теста в наборе `TerekhovDFastSortBatchPerfTests`

### Тестовые сценарии (13 различных случаев):

```cpp
// Базовые тесты сортировки
case_01: {15, 8, 3, 12, 6} → {3, 6, 8, 12, 15}
case_02: {-7, 25, 0, -3, 14, -1} → {-7, -3, -1, 0, 14, 25}
case_03: {9, 9, 5, 5, 7, 2, 2} → {2, 2, 5, 5, 7, 9, 9}
case_04: {2, 4, 6, 8, 7, 9} → {2, 4, 6, 7, 8, 9}
case_05: {12, 11, 10, 9, 8, 7, 6, 5, 4} → {4, 5, 6, 7, 8, 9, 10, 11, 12}
case_06: {99} → {99}
case_07: {15, 3, 9, 5, 12, 7, 15, 1, 22, 15, 5} → {1, 3, 5, 5, 7, 9, 12, 15, 15, 15, 22}
case_08: {30, -8, 45, -20, 7, 2, 18} → {-20, -8, 2, 7, 18, 30, 45}
case_09: {} → {}
case_10: {33, 17} → {17, 33}
case_11: Большой массив из 50 элементов с отрицательными числами
case_12: Массив из 50 элементов со специальными значениями
case_13: Массив из 60 элементов со сложным распределением
```
### Результаты проверки корректности:

- Все 26 функциональных тестов пройдены успешно

- SEQ и MPI реализации дают идентичные результаты

- Все тесты производительности пройдены

- Общее количество пройденных тестов в рамках проекта: 59 тестов

### 7.2 Производительность
**Параметры тестирования:**
- Размер тестового массива: 75 000 000 элементов

- Тип элементов: int

- Характер данных: случайные числа с некоторыми специальными значениями (отрицательные, нули, дубликаты)"

- Измерения проводились для 1, 2 и 4 процессов

### Результаты измерения производительности:

**Время выполнения (task_run) - чистые вычисления**
| Режим | Процессы | Время, с | Ускорение | Эффективность |
|-------|----------|----------|-----------|---------------|
| seq   | 1        | 4.9453  | 1.00      | 100%          |
| mpi   | 1        | 5.3298  | 0.93     | 93%           |
| mpi   | 2        | 3.2965   | 1.50    | 75%           |
| mpi   | 4        | 	2.0027  | 2.47     | 62%           |
### Время выполнения (pipeline) - полный цикл
| Режим | Процессы | Время, с | Ускорение | Эффективность |
|-------|----------|----------|-----------|---------------|
| seq   | 1        | 4.9203   | 1.00      | 100%          |
| mpi   | 1        | 5.2871  | 0.93     | 93%           |
| mpi   | 2        | 	3.2921 | 1.49     | 75%           |
| mpi   | 4        | 2.0501  | 2.40     | 60%           |
### Сравнение SEQ и MPI при 1 процессе
| Метрика | SEQ  |  MPI(1 процесс) | Разница |
|------------|----------|----------|-----------|
| task_run   | 4.9453 с    | 5.3298 с  | 0.93× (замедление 7%)     |
| pipeline   | 4.9203 с    | 5.2871 с  | 0.93× (замедление 7%)    |

**Анализ результатов:**

**1. Основные наблюдения:**

- SEQ vs MPI с 1 процессом: MPI версия немного медленнее (7% замедление) из-за накладных расходов MPI

- Масштабирование MPI: наблюдается положительное масштабирование с увеличением числа процессов

- Оптимальная конфигурация: 4 процесса дают наилучшее ускорение (2.47×)

**2. Ускорение и эффективность:**

- Лучшее ускорение: 2.47× для task_run с 4 процессами

- Эффективность: от 60% до 93% (хорошее масштабирование)

- Масштабируемость: линейное улучшение производительности с увеличением числа процессов

**3. Сравнение task_run и pipeline:**

- task_run: показывает лучшую абсолютную производительность

- pipeline: демонстрирует схожие тенденции масштабирования

- Разница между SEQ и MPI на 1 процессе: **небольшое замедление MPI**

**Ключевые выводы:**

- MPI версия демонстрирует хорошее масштабирование - ускорение до 2.47× на 4 процессах

- На 1 процессе MPI немного медленнее SEQ (7% замедление) из-за накладных расходов MPI

- Оптимальное количество процессов - 4 с ускорением 2.47×

- Эффективность параллелизации составляет 60-75%, что является хорошим результатом для алгоритма сортировки

## 8. Выводы
### Достижения
**1. Функциональная корректность:**

- Все 59 тестов пройдены успешно

- SEQ и MPI версии дают идентичные результаты

- Реализован полноценный алгоритм четно-нечетного слияния Бэтчера

**2. Архитектурная корректность:**
- Правильное использование MPI API

- Корректное распределение данных между процессами

### Ограничения и проблемы

**1. Производительность:**

- MPI версия демонстрирует **хорошее масштабирование** до 4 процессов

- **Накладные расходы MPI** на 1 процессе приводят к небольшому замедлению

- **Эффективность параллелизации** хорошая (60-75%)

**Ограничения алгоритма Бэтчера:**

- Высокие коммуникационные затраты

- Алгоритм оптимален для сетей сортировки, но не для традиционных MPI

- Требует множественных синхронных обменов данными

### Общий вывод
Реализованный алгоритм быстрой сортировки с четно-нечетным слиянием Бэтчера является корректным с функциональной точки зрения и демонстрирует хорошее масштабирование MPI версии (до 2.47× ускорения на 4 процессах). Несмотря на небольшие накладные расходы на 1 процессе, параллельная версия показывает значительное преимущество при использовании нескольких процессов.

## 9. Список литературы

1. Сысоев А. В. Лекции по параллельному программированию. — Н. Новгород: ННГУ, 2025. 
2. Документация Open MPI     https://www.open-mpi.org/doc/
3. Microsoft Функции MPI     https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-functions