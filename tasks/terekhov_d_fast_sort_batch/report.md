# Ленточная горизонтальная схема - умножение матрицы на вектор
- **Студент**: Терехов Дмитрий Юрьевич 
- **группа:** 3823Б1ПР4
- **Технология**: SEQ | MPI
- **Вариант**: 15 (Быстрая сортировка с четно-нечетным слиянием Бэтчера.)

## 1. Введение
Сортировка является одной из фундаментальных операций в информатике, используемой в широком спектре приложений от баз данных до научных вычислений. Быстрая сортировка (Quicksort) - один из наиболее эффективных алгоритмов сортировки со средней сложностью O(n log n). Четно-нечетное слияние Бэтчера (Batcher's odd-even merge) - это параллельный алгоритм слияния, который особенно эффективен в параллельных системах благодаря своей регулярной структуре сравнений и обменов.

## 2. Постановка задачи

**Цель работы:**
Реализовать и сравнить производительность последовательного (SEQ) и параллельного (MPI) алгоритмов быстрой сортировки с четно-нечетным слиянием Бэтчера, где массив распределяется между процессами, каждый процесс выполняет локальную сортировку, а затем применяется алгоритм Бэтчера для глобального слияния.

**Определение задачи:**

Для заданного массива целых чисел размером N необходимо отсортировать его в порядке возрастания, используя:

- Последовательный алгоритм быстрой сортировки с четно-нечетным слиянием

- Параллельный алгоритм с распределением данных между процессами MPI

**Ограничения:**

- Входные данные - вектор целых чисел произвольного размера

- Для параллельной реализации используется MPI с алгоритмом четно-нечетного слияния Бэтчера

- Данные распределяются примерно равными частями между процессами

- Результат обеих реализаций (последовательной и параллельной) должен совпадать

## 3. Базовый алгоритм

### Алгоритм последовательной реализации

1. Инициализация
- Получить на вход вектор данных

- Инициализировать результирующий вектор
2. Умножение матрицы на вектор
- Выбор опорного элемента как медианы трех (первый, средний, последний)

- Разделение массива относительно опорного элемента

- Рекурсивная сортировка левой и правой частей (используется стек вместо рекурсии)
3. Четно-нечетное слияние
- Для массивов размера ≤ 2 - простой обмен

- Для больших массивов - стандартное двухпутевое слияние

**Код последовательной реализации (ключевая часть):**

```cpp
bool TerekhovDFastSortBatchSEQ::RunImpl() {
  std::vector<int> data_array = GetInput();
  if (data_array.empty()) {
    return true;
  }

  struct StackElement {
    int left;
    int right;
  };
  
  std::vector<StackElement> call_stack;
  call_stack.push_back({0, static_cast<int>(data_array.size()) - 1});

  while (!call_stack.empty()) {
    StackElement current = call_stack.back();
    call_stack.pop_back();

    int l = current.left;
    int r = current.right;

    if (l >= r) {
      continue;
    }

    auto [new_left, new_right] = PartitionSegment(data_array, l, r);

    if (l < new_right) {
      call_stack.push_back({l, new_right});
    }
    if (new_left < r) {
      call_stack.push_back({new_left, r});
    }
  }

  GetOutput().swap(data_array);
  return true;
}
```

## 4. Схема распараллеливания
### 4.1. Распределение данных
**Для параллельной обработки используется алгоритм Бэтчера:**

1. Распределение массива:

- Массив разбивается на сегменты по процессам

- Каждый процесс получает примерно равный сегмент

- Для N элементов и P процессов: базовый размер = N/P, остаток распределяется первым процессам

2. Локальная сортировка:

- Каждый процесс сортирует свой сегмент быстрой сортировкой

3. Глобальное слияние Бэтчера:

- Применяются четные и нечетные фазы обмена

- В четных фазах обмениваются процессы с четными номерами

- В нечетных фазах обмениваются процессы с нечетными номерами

### 4.2. Схема связи и топология
**Используется звездообразная топология с процессом 0 в качестве координатора:**
**Нисходящие связи (от процесса 0 к worker-процессам):**

- Рассылка размеров массива `(MPI_Bcast)`

- Рассылка всего массива `(MPI_Bcast)`

- Распределение границ сегментов `(MPI_Send)`

**Восходящие связи (от worker-процессов к процессу 0):**

- Передача отсортированных сегментов `(MPI_Send)`

**Горизонтальные связи (между процессами для алгоритма Бэтчера):**

- Обмен данными между соседними процессами `(MPI_Sendrecv)`


### 4.3. Ранжирование ролей    
**Процесс 0 (Master-координатор):**

1. Инициализация и рассылка данных

2. Распределение сегментов между процессами

3. Локальная сортировка своего сегмента

4. Координация фаз алгоритма Бэтчера

5. Сбор результатов от всех процессов

6. Рассылка финального результата

**Процессы 1..N-1 (Worker-процессы):**

1. Получение данных и границ сегментов

2. Локальная сортировка своего сегмента

3. Участие в фазах алгоритма Бэтчера

4. Отправка результатов процессу 0

5. Получение финального результата

### 4.4. Псевдокод алгоритма Бэтчера
```cpp
function BatcherMergeProcedure(local_vec, segment, arr_size, rank, proc_count):
    min_segment_size = arr_size / proc_count
    if min_segment_size == 0: min_segment_size = 1
    
    phases = (arr_size + min_segment_size - 1) / min_segment_size
    
    for phase = 0 to phases-1:
        if phase % 2 == 0:      // Четная фаза
            if rank четный и есть правый сосед:
                обмен_с_правым_соседом()
            elif rank нечетный:
                обмен_с_левым_соседом()
        else:                   // Нечетная фаза
            if rank нечетный и есть правый сосед:
                обмен_с_правым_соседом()
            elif rank четный и не первый:
                обмен_с_левым_соседом()

function обмен_с_соседом(сосед_смещение):
    отправить_данные_соседу()
    получить_данные_от_соседа()
    объединить_и_выбрать_меньшую_часть()
```
## 5. Детали реализации
### Структура кода
**Файловая структура:**
```cpp
terekhov_d_fast_sort_batch/
├── common/include/common.hpp
├── seq/include/ops_seq.hpp
├── seq/src/ops_seq.cpp
├── mpi/include/ops_mpi.hpp
├── mpi/src/ops_mpi.cpp
├── tests/functional/main.cpp
├── tests/performance/main.cpp
└── settings.json
```
**Ключевые классы:**

**1. Последовательная реализация (seq):**

- `TerekhovDFastSortBatchSEQ` - класс последовательной реализации

- Основной метод: `RunImpl()` - быстрая сортировка с итеративным стеком

**2. MPI реализация (`mpi`):**

- `TerekhovDFastSortBatchMPI` - класс параллельной реализации

- **Основные методы:**

- - `RunImpl()` - основной алгоритм с распределением данных

- - `BroadcastInputData()` - рассылка входных данных

- - `GetProcessSegment()` - вычисление границ сегментов

- - `SortLocalData()` - локальная быстрая сортировка

- - `BatcherMergeProcedure()` - алгоритм Бэтчера

- - `BatcherEvenStep()` & `BatcherOddStep()`- четные/нечетные фазы

- - `ProcessExchange()` -  обмен данными между процессами

- - `CombineAndSelect()` - слияние и выбор части данных

**Архитектурные особенности:**

- Использование MPI для межпроцессного взаимодействия
- Алгоритм четно-нечетного слияния Бэтчера для глобального слияния
- Централизованная модель с процессом-координатором (rank 0)
- Улучшенный выбор опорного элемента (медиана трех)
- Итеративная реализация быстрой сортировки вместо рекурсивной

### Важные допущения и ограничения
**Обработка данных:**

- Поддержка массивов произвольного размера
- Использование типа int для элементов
- Проверка корректности в `ValidationImpl()`

**Граничные случаи:**

- Пустой массив
- Массивы из одного элемента
- Уже отсортированные массивы
- Очень большие массивы (75 миллионов элементов в тестах производительности)

## 6. Экспериментальная установка
### Аппаратное обеспечение и ОС
**Системные характеристики:**

- Модель процессора: intel core i5 10400f
- Архитектура: x86_64
- Ядра/потоки: 6 ядер 12 потоков
- Оперативная память: 32 GB 
- Операционная система: Windows 10
- Тип системы: PC (personal computer)

### Набор инструментов
**Компиляция и сборка:**

- Компиляторы: GCC и Clang
- Среда разработки: Visual Studio Code
- Стандарт языка: C++17
- Система сборки: CMake
- Тип сборки: Release

### Управление процессами
Запуск тестов:
```cpp
bash

# Функциональные тесты на 4 процессах
mpirun --allow-run-as-root -n 4 ./build/bin/ppc_func_tests

# Тесты производительности на 4 процессах  
mpirun --allow-run-as-root -n 4 ./build/bin/ppc_perf_tests

# Тесты на 2 процессах
mpirun --allow-run-as-root -n 2 ./build/bin/ppc_func_tests
mpirun --allow-run-as-root -n 2 ./build/bin/ppc_perf_tests
```

## 7. Результаты и обсуждение
### 7.1 Корректность
**Методы проверки корректности:**

1. Функциональное тестирование: 26 тестов в наборе `TerekhovDFastSortBatchFuncTests`

2. Тестирование производительности: 4 теста в наборе `TerekhovDFastSortBatchPerfTests`

### Тестовые сценарии (13 различных случаев):

```cpp
// Базовые тесты сортировки
case_01: {15, 8, 3, 12, 6} → {3, 6, 8, 12, 15}
case_02: {-7, 25, 0, -3, 14, -1} → {-7, -3, -1, 0, 14, 25}
case_03: {9, 9, 5, 5, 7, 2, 2} → {2, 2, 5, 5, 7, 9, 9}
case_04: {2, 4, 6, 8, 7, 9} → {2, 4, 6, 7, 8, 9}
case_05: {12, 11, 10, 9, 8, 7, 6, 5, 4} → {4, 5, 6, 7, 8, 9, 10, 11, 12}
case_06: {99} → {99}
case_07: {15, 3, 9, 5, 12, 7, 15, 1, 22, 15, 5} → {1, 3, 5, 5, 7, 9, 12, 15, 15, 15, 22}
case_08: {30, -8, 45, -20, 7, 2, 18} → {-20, -8, 2, 7, 18, 30, 45}
case_09: {} → {}
case_10: {33, 17} → {17, 33}
case_11: Большой массив из 50 элементов с отрицательными числами
case_12: Массив из 50 элементов со специальными значениями
case_13: Массив из 60 элементов со сложным распределением
```
### Результаты проверки корректности:

- Все 26 функциональных тестов пройдены успешно

- SEQ и MPI реализации дают идентичные результаты

- Все тесты производительности пройдены

- Общее количество пройденных тестов в рамках проекта: 59 тестов

### 7.2 Производительность
**Параметры тестирования:**
- Размер тестового массива: 75 000 000 элементов

- Тип элементов: int

- Характер данных: обратно отсортированный массив (от 74 999 999 до 0)

- Измерения проводились для 1, 2 и 4 процессов

### Результаты измерения производительности:

**Время выполнения (task_run) - чистые вычисления**
| Режим | Процессы | Время, с | Ускорение | Эффективность |
|-------|----------|----------|-----------|---------------|
| seq   | 1        | 1.4512  | 1.00      | 100%          |
| mpi   | 1        | 1.4512  | 1.00     | 100%           |
| mpi   | 2        | 2.1391   | 0.68    | 34%           |
| mpi   | 4        | 2.9309  | 0.50     | 12%           |
### Время выполнения (pipeline) - полный цикл
| Режим | Процессы | Время, с | Ускорение | Эффективность |
|-------|----------|----------|-----------|---------------|
| seq   | 1        | 1.4491   | 1.00      | 100%          |
| mpi   | 1        | 1.5681   | 0.92     | 92%           |
| mpi   | 2        | 	1.3126 | 1.10     | 55%           |
| mpi   | 4        | 1.5681  | 0.92     | 23%           |
### Сравнение SEQ и MPI при 1 процессе
| Метрика | SEQ  |  MPI(1 процесс) | Разница |
|------------|----------|----------|-----------|
| task_run   | 1.4512 с    | 1.4512 с  | 0%     |
| pipeline   | 1.4491 с    | 1.5681 с  | +8.2%    |

**Анализ результатов:**

**1. Основные наблюдения:**

- SEQ vs MPI с 1 процессом: практически идентичная производительность для task_run, небольшое преимущество у SEQ в pipeline (8.2%) из-за накладных расходов MPI

- Масштабирование MPI: наблюдается отрицательное масштабирование при увеличении числа процессов для task_run

- Оптимальная конфигурация: 2 процесса для pipeline дают наилучшее ускорение (1.10×)

**2. Ускорение и эффективность:**

- Лучшее ускорение: 1.10× для pipeline с 2 процессами

- Эффективность: от 12% до 55% в зависимости от режима и числа процессов

- Парадокс task_run: увеличение времени с ростом числа процессов

**3. Сравнение task_run и pipeline:**

- task_run: показывает отрицательное масштабирование из-за сложности алгоритма Бэтчера

- pipeline: демонстрирует более стабильное поведение с пиком на 2 процессах

- Разница: объясняется разными подходами к измерению времени (чистые вычисления vs полный цикл)

**Ключевые выводы:**

1. Для данного размера данных (75 миллионов) MPI версия не демонстрирует значительного преимущества

2. Алгоритм Бэтчера создает значительные коммуникационные накладные расходы

3. Оптимальная конфигурация для pipeline - 2 процесса

4. SEQ реализация остается наиболее эффективной для одиночных вычислений

## 8. Выводы
### Достижения
**1. Функциональная корректность:**

- Все 59 тестов пройдены успешно

- SEQ и MPI версии дают идентичные результаты

- Реализован полноценный алгоритм четно-нечетного слияния Бэтчера

**2. Архитектурная корректность:**

- Правильное использование MPI API

- Корректное распределение данных между процессами

- Эффективная реализация быстрой сортировки с медианой трех

### Ограничения и проблемы
**1. Производительность:**

- MPI версия демонстрирует ограниченное ускорение (максимум 1.10×)

- Отрицательное масштабирование для task_run при увеличении числа процессов

- Эффективность: 12-55% от теоретического максимума

**Ограничения алгоритма Бэтчера:**

- Высокие коммуникационные затраты

- Алгоритм оптимален для сетей сортировки, но не для традиционных MPI

- Требует множественных синхронных обменов данными

### Общий вывод
Реализованный алгоритм быстрой сортировки с четно-нечетным слиянием Бэтчера является корректным с функциональной точки зрения и демонстрирует работоспособность параллельной реализации.

## 9. Список литературы

1. Сысоев А. В. Лекции по параллельному программированию. — Н. Новгород: ННГУ, 2025.
2. Документация Open MPI     https://www.open-mpi.org/doc/
3. Microsoft Функции MPI     https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-functions