# Нахождение минимальных значений по столбцам матрицы

- **Студент**: Терехов Дмитрий Юрьевич 
- **группа:** 3823Б1ПР4
- **Технология**: SEQ | MPI
- **Вариант**: **18** (Нахождение минимальных значений по столбцам матрицы)

## 1. Введение
В научных вычислениях и обработке данных часто возникает задача анализа матриц, включая поиск экстремальных значений по различным направлениям. Нахождение минимальных значений по столбцам матрицы является фундаментальной операцией в линейной алгебре, статистическом анализе и машинном обучении. Для больших матриц последовательный алгоритм может быть недостаточно эффективным, что требует применения параллельных вычислений для ускорения обработки данных.

## 2. Постановка задачи
Дана матрица размером m×n, состоящая из целых чисел.
Требуется найти минимальное значение в каждом столбце матрицы.

Тип входных данных:
```cpp
using InType = std::vector<std::vector<int>>;
```
Тип выходных данных:
```cpp
using OutType = std::vector<int>;
```
Ограничения:
- Матрица может быть прямоугольной (m ≠ n)
- Матрица может содержать как положительные, так и отрицательные числа
- Все строки матрицы должны иметь одинаковую длину
- Матрица не может быть пустой

## 3. Базовый алгоритм (Sequential)
Алгоритм последовательного нахождения минимумов по столбцам:
```cpp
OutType result(cols, std::numeric_limits<int>::max());

for (std::size_t r = 0; r < rows; ++r) {
  for (std::size_t c = 0; c < cols; ++c) {
    if (matrix[r][c] < result[c]) {
      result[c] = matrix[r][c];
    }
  }
}
```
Алгоритм инициализирует результат максимальными значениями для каждого столбца, затем последовательно проходит по всем элементам матрицы, обновляя минимальные значения для соответствующих столбцов.

## 4. Схема распараллеливания
### Краткое описание
1. Нулевой процесс получает исходную матрицу и определяет её размеры (количество строк и столбцов)

2. Размеры матрицы передаются всем процессам через MPI_Bcast()

3. Строки матрицы равномерно распределяются между процессами с учетом остатка

4. Исходная матрица разбивается на блоки строк и распределяется по процессам с помощью MPI_Scatterv()

5. Каждый процесс находит локальные минимумы по столбцам для своих строк

6. Все процессы обмениваются локальными минимумами через MPI_Allreduce() с операцией MPI_MIN

7. Полученный результат сохраняется как выходные данные

### Алгоритм распределения строк по процессам
```cpp
std::vector<int> counts_rows(world_size_, 0);
std::vector<int> displs_rows(world_size_, 0);
int base = rows / world_size_;
int rem = rows % world_size_;

for (int r = 0; r < world_size_; ++r) {
  counts_rows[r] = base + (r < rem ? 1 : 0);
}
for (int r = 1; r < world_size_; ++r) {
  displs_rows[r] = displs_rows[r - 1] + counts_rows[r - 1];
}
```
### Алгоритм нахождения локальных минимумов
```cpp
std::vector<int> local_min(cols_, std::numeric_limits<int>::max());

for (int r = 0; r < local_rows_; ++r) {
  int base_idx = r * static_cast<int>(cols_);
  for (std::size_t c = 0; c < cols_; ++c) {
    int v = local_buffer_[base_idx + static_cast<int>(c)];
    if (v < local_min[c]) {
      local_min[c] = v;
    }
  }
}
```
### Схема параллельной работы алгоритма
1. Каждый процесс определяет свой ранг и общее количество процессов через **MPI_Comm_rank()** и **MPI_Comm_size()**

2. Нулевой процесс валидирует входные данные и передает результат валидации всем процессам через **MPI_Bcast()**

3. Размеры матрицы (количество строк и столбцов) передаются от нулевого процесса всем остальным через **MPI_Bcast()**

4. Вычисляется распределение строк по процессам с балансировкой нагрузки

5. Матрица преобразуется в плоский массив и распределяется по процессам с помощью **MPI_Scatterv()**

6. Каждый процесс вычисляет локальные минимумы по столбцам для своих строк

7. Все процессы обмениваются результатами через **MPI_Allreduce()** для получения глобальных минимумов

8. Результат сохраняется в выходных данных

## 5. Детали реализации
|          Файл          |                 Назначение                  | 
|------------------------|---------------------------------------------|
| `common.hpp`           | Определение типов входных и выходных данных, базового класса задачи |
| `ops_seq.hpp/.cpp`     |         Последовательная реализация алгоритма  |
| `ops_mpi.hpp/.cpp`     |               	MPI-реализация алгоритма        |
| `functional/main.cpp`  |             	Функциональные тесты для проверки корректности          |
| `performance/main.cpp` |      Тесты производительности для измерения эффективности       |

### 5.1 Функциональные тесты
Функциональные тесты реализованы с использованием Google Test и покрывают следующие случаи:

1. **Прямоугольная матрица 3×3** - проверка работы с обычной матрицей
```cpp
input_data_ = {{3, 5, -1}, {0, -2, 10}, {7, 1, -5}};
expected_ = {0, -2, -5};
```
2. **Матрица с одной строкой 1×3** - проверка граничного случая
```cpp
input_data_ = {{4, -1, 2}};
expected_ = {4, -1, 2};
```
3. **Матрица с отрицательными числами 3×2** - проверка работы с отрицательными значениями
```cpp
input_data_ = {{-10, 3}, {-5, 2}, {-20, 4}};
expected_ = {-20, 2};
```
### 5.2 Тесты производительности
Тесты производительности используют матрицу размером **15000×15000** со случайными целыми числами в диапазоне от **1** до **1000000**. Фиксированный seed (42) обеспечивает воспроизводимость результатов.

## 6. Экспериментальная среда
|  Компонент |               Значение                       |
|------------|----------------------------------------------|
|     CPU    |           intel core i5 10400f                |
|     RAM    |                 32 GB                       |
|     ОС     | OS: Ubuntu 24.04 (DevContainer / Windows 10) |
| Компилятор | GCC 13.3.0 (g++), C++20, CMake, Release     |
|     MPI    |        mpirun (Open MPI) 4.1.6            |

## 7. Результаты и обсуждение
### 7.1 Корректность
Функциональные тесты успешно пройдены как для SEQ, так и для MPI реализаций:

- Все 6 тестов (3 случая × 2 реализации) выполнены без ошибок
- Время выполнения каждого теста не превышает 1 мс
- Результаты полностью совпадают с ожидаемыми значениями

### 7.2 Производительность

Для тестов производительности используется матрица **15000×15000** (225 миллионов элементов).
| Режим | Процессы | Время, сек | Ускорение | Эффективность |
|-------|----------|------------|-----------|---------------|
| SEQ (pipeline) | 1 | 0.096565 | 1.00 | 100% |
| SEQ (task_run) | 1 | 0.096825 | 1.00 | 100% |
| MPI (pipeline) | 2 | 0.822729 | 0.12 | 6% |  
| MPI (task_run) | 2 | 0.101745 | 0.95 | 48% |
| MPI (pipeline) | 4 | 0.808393 | 0.12 | 3% |
| MPI (task_run) | 4 | 0.055778 | 1.74 | 44% |

1. **Сравнение режимов выполнения:**
   - `task_run` режим показывает значительно лучшую производительность по сравнению с `pipeline`
   - Разница особенно заметна для MPI реализации: `pipeline` включает дополнительные этапы валидации и подготовки данных

2. **Масштабируемость MPI реализации:**
   - При использовании 2 процессов в `task_run` режиме достигнуто ускорение 0.95× с эффективностью 48%
   - При использовании 4 процессов в `task_run` режиме достигнуто ускорение 1.74× с эффективностью 44%
   - Наблюдается положительное масштабирование при увеличении числа процессов с 2 до 4

3. **Влияние коммуникационных затрат:**
   - Для 2 и 4 процессов в `pipeline` режиме наблюдаются значительные накладные расходы (эффективность 6% и 3% соответственно)
   - В `task_run` режиме коммуникационные затраты минимизированы, что позволяет достичь лучших результатов
   - Переход от 2 к 4 процессам в `task_run` режиме дает дополнительное ускорение в 1.83 раза

4. **Оптимальная конфигурация:**
   - Наиболее эффективной является MPI реализация с 4 процессами в `task_run` режиме
   - Для данной задачи размером 15000×15000 использование 4 процессов дает ускорение в 1.74 раза по сравнению с последовательной версией

**Экспериментальные наблюдения:**
- Алгоритм демонстрирует хорошую масштабируемость для больших матриц
- Основной выигрыш от параллелизации достигается при обработке строк матрицы
- Операция `MPI_Allreduce` с `MPI_MIN` эффективно агрегирует результаты без необходимости дополнительной синхронизации
- Pipeline режим имеет значительные накладные расходы, особенно для небольших матриц

## 8. Заключение
Разработанный параллельный алгоритм нахождения минимальных значений по столбцам матрицы доказал свою эффективность на практике. При обработке матрицы 15000×15000 достигнуто ускорение в 1.74 раза при использовании 4 процессов, что подтверждает практическую ценность применения MPI для данных вычислений. Эксперименты показали положительное масштабирование алгоритма при увеличении числа процессов с 2 до 4. Работа демонстрирует успешное применение знаний по параллельному программированию для решения реальной вычислительной задачи.

## 9. Источники
1. Сысоев А. В. Курс лекций по параллельному программированию
2. Документация Open MPI     https://www.open-mpi.org/doc/
3. Microsoft Функции MPI     https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-functions