# Нахождение минимальных значений по столбцам матрицы

- **Студент**: Терехов Дмитрий Юрьевич 
- **группа:** 3823Б1ПР4
- **Технология**: SEQ | MPI
- **Вариант**: **18** (Нахождение минимальных значений по столбцам матрицы)

## 1. Введение
В научных вычислениях и обработке данных часто возникает задача анализа матриц, включая поиск экстремальных значений по различным направлениям. Нахождение минимальных значений по столбцам матрицы является фундаментальной операцией в линейной алгебре, статистическом анализе и машинном обучении. Для больших матриц последовательный алгоритм может быть недостаточно эффективным, что требует применения параллельных вычислений для ускорения обработки данных.

## 2. Постановка задачи
Дана матрица размером m×n, состоящая из целых чисел.
Требуется найти минимальное значение в каждом столбце матрицы.

Тип входных данных:
```cpp
using InType = std::vector<std::vector<int>>;
```
Тип выходных данных:
```cpp
using OutType = std::vector<int>;
```
Ограничения:
- Матрица может быть прямоугольной (m ≠ n)
- Матрица может содержать как положительные, так и отрицательные числа
- Все строки матрицы должны иметь одинаковую длину
- Матрица не может быть пустой

## 3. Базовый алгоритм (Sequential)
Алгоритм последовательного нахождения минимумов по столбцам:
```cpp
OutType result(cols, std::numeric_limits<int>::max());

for (std::size_t r = 0; r < rows; ++r) {
  for (std::size_t c = 0; c < cols; ++c) {
    if (matrix[r][c] < result[c]) {
      result[c] = matrix[r][c];
    }
  }
}
```
Алгоритм инициализирует результат максимальными значениями для каждого столбца, затем последовательно проходит по всем элементам матрицы, обновляя минимальные значения для соответствующих столбцов.

## 4. Схема распараллеливания
### Краткое описание
1. Нулевой процесс получает исходную матрицу и определяет её размеры (количество строк и столбцов)

2. Размеры матрицы передаются всем процессам через MPI_Bcast()

3. Строки матрицы равномерно распределяются между процессами с учетом остатка

4. Исходная матрица разбивается на блоки строк и распределяется по процессам с помощью MPI_Scatterv()

5. Каждый процесс находит локальные минимумы по столбцам для своих строк

6. Все процессы обмениваются локальными минимумами через MPI_Allreduce() с операцией MPI_MIN

7. Полученный результат сохраняется как выходные данные

### Алгоритм распределения строк по процессам
```cpp
std::vector<int> counts_rows(world_size_, 0);
std::vector<int> displs_rows(world_size_, 0);
int base = rows / world_size_;
int rem = rows % world_size_;

for (int r = 0; r < world_size_; ++r) {
  counts_rows[r] = base + (r < rem ? 1 : 0);
}
for (int r = 1; r < world_size_; ++r) {
  displs_rows[r] = displs_rows[r - 1] + counts_rows[r - 1];
}
```
### Алгоритм нахождения локальных минимумов
```cpp
std::vector<int> local_min(cols_, std::numeric_limits<int>::max());

for (int r = 0; r < local_rows_; ++r) {
  int base_idx = r * static_cast<int>(cols_);
  for (std::size_t c = 0; c < cols_; ++c) {
    int v = local_buffer_[base_idx + static_cast<int>(c)];
    if (v < local_min[c]) {
      local_min[c] = v;
    }
  }
}
```
### Схема параллельной работы алгоритма
1. Каждый процесс определяет свой ранг и общее количество процессов через **MPI_Comm_rank()** и **MPI_Comm_size()**

2. Нулевой процесс валидирует входные данные и передает результат валидации всем процессам через **MPI_Bcast()**

3. Размеры матрицы (количество строк и столбцов) передаются от нулевого процесса всем остальным через **MPI_Bcast()**

4. Вычисляется распределение строк по процессам с балансировкой нагрузки

5. Матрица преобразуется в плоский массив и распределяется по процессам с помощью **MPI_Scatterv()**

6. Каждый процесс вычисляет локальные минимумы по столбцам для своих строк

7. Все процессы обмениваются результатами через **MPI_Allreduce()** для получения глобальных минимумов

8. Результат сохраняется в выходных данных

## 5. Детали реализации
|          Файл          |                 Назначение                  | 
|------------------------|---------------------------------------------|
| `common.hpp`           | Определение типов входных и выходных данных, базового класса задачи |
| `ops_seq.hpp/.cpp`     |         Последовательная реализация алгоритма  |
| `ops_mpi.hpp/.cpp`     |               	MPI-реализация алгоритма        |
| `functional/main.cpp`  |             	Функциональные тесты для проверки корректности          |
| `performance/main.cpp` |      Тесты производительности для измерения эффективности       |

### 5.1 Функциональные тесты
Функциональные тесты реализованы с использованием Google Test и покрывают следующие случаи:

1. **Прямоугольная матрица 3×3** - проверка работы с обычной матрицей
```cpp
input_data_ = {{3, 5, -1}, {0, -2, 10}, {7, 1, -5}};
expected_ = {0, -2, -5};
```
2. **Матрица с одной строкой 1×3** - проверка граничного случая
```cpp
input_data_ = {{4, -1, 2}};
expected_ = {4, -1, 2};
```
3. **Матрица с отрицательными числами 3×2** - проверка работы с отрицательными значениями
```cpp
input_data_ = {{-10, 3}, {-5, 2}, {-20, 4}};
expected_ = {-20, 2};
```
4. **Матрица с одним элементом 1×1** - минимальный случай
```cpp
input_data_ = {{7}};
expected_ = {7};
```
5. **Квадратная матрица 2×2** - проверка работы с квадратной матрицей
```cpp
input_data_ = {{1, 2}, {3, 4}};
expected_ = {1, 2};
```
6. **Матрица с одной строкой и двумя столбцами** - проверка горизонтальной матрицы
```cpp
input_data_ = {{10, 20}};
expected_ = {10, 20};
```
7. **Матрица с одним столбцом и тремя строками** - проверка вертикальной матрицы
```cpp
input_data_ = {{3}, {1}, {2}};
expected_ = {1};
```
8. **Матрица с одинаковыми значениями** - проверка работы при совпадении минимумов
```cpp
input_data_ = {{5, 5, 5}, {5, 5, 5}};
expected_ = {5, 5, 5};
```
9. **Матрица с отрицательными значениями в одном столбце** - проверка работы с отрицательными числами
```cpp
input_data_ = {{-3}, {-1}, {-2}, {-5}};
expected_ = {-5};
```
10. **Матрица со смешанными значениями** - проверка общего случая
```cpp
input_data_ = {{10, 20}, {30, 5}, {15, 25}};
expected_ = {10, 5};
```

### 5.2 Тесты производительности
Тесты производительности используют матрицу размером **5000×5000** со случайными целыми числами в диапазоне от **1** до **1000000**. Фиксированный seed (42) обеспечивает воспроизводимость результатов.

## 6. Экспериментальная среда
|  Компонент |               Значение                       |
|------------|----------------------------------------------|
|     CPU    |           intel core i5 10400f                |
|     RAM    |                 32 GB                       |
|     ОС     | OS: Ubuntu 24.04 (DevContainer / Windows 10) |
| Компилятор | GCC 13.3.0 (g++), C++20, CMake, Release     |
|     MPI    |        mpirun (Open MPI) 4.1.6            |

## 7. Результаты и обсуждение
### 7.1 Корректность
Функциональные тесты успешно пройдены как для SEQ, так и для MPI реализаций:

- Все 20 тестов (10 случаев × 2 реализации) выполнены без ошибок
- Время выполнения каждого теста не превышает 1 мс
- Результаты полностью совпадают с ожидаемыми значениями

### 7.2 Производительность

Для тестов производительности используется матрица **5000×5000** (25 миллионов элементов).
| Режим | Процессы | Время, сек | Ускорение | Эффективность |
|-------|----------|------------|-----------|---------------|
| SEQ (pipeline) | 1 | 0.0257 | 1.00 | 100% |
| SEQ (task_run) | 1 | 0.0281 | 1.00 | 100% |
| MPI (pipeline) | 2 | 0.0762 | 0.37 | 19% |  
| MPI (task_run) | 2 | 0.0703 | 0.40 | 20% |
| MPI (pipeline) | 4 | 0.0879 | 0.32 | 8% |
| MPI (task_run) | 4 | 0.0822 | 0.34 | 9% |
| MPI (pipeline) | 7 | 0.0835 | 0.34 | 5% |
| MPI (task_run) | 7 | 0.0736 | 0.38 | 5% |
| MPI (pipeline) | 8 | 0.0872 | 0.32 | 4% |
| MPI (task_run) | 8 | 0.0711 | 0.40 | 5% |

**Анализ результатов производительности:**

1. **Сравнение pipeline и task_run режимов:**
   - Для SEQ реализации: `pipeline` режим на 9% быстрее `task_run`, что объясняется исключением дополнительных вызовов методов-заглушек
   - Для MPI реализации: `task_run` режим на 7-8% быстрее `pipeline` для всех конфигураций
   - Разница между режимами незначительна и составляет 7-9%, что подтверждает минимальность накладных расходов на дополнительные этапы обработки в `pipeline` режиме

2. **Сравнение SEQ и MPI реализаций:**
   - MPI реализация оказалась в 2.5-3.1 раза медленнее SEQ реализации
   - Основная причина: коммуникационные затраты (`MPI_Scatterv`, `MPI_Allreduce`) превосходят выигрыш от параллельной обработки строк
   - Операция `MPI_Allreduce` требует обмена данными между всеми процессами для каждого из 5000 столбцов, что создает значительные накладные расходы

3. **Масштабируемость MPI реализации:**
   - При увеличении числа процессов с 2 до 4 производительность не улучшается, а ухудшается
   - При 7-8 процессах время выполнения сопоставимо с результатами для 2-4 процессов
   - Наблюдается **отсутствие масштабируемости** из-за доминирования коммуникационных затрат

4. **Ключевые выводы:**
   - Для задачи поиска минимумов по столбцам матрицы размером 5000×5000 последовательная реализация эффективнее параллельной
   - Основной ограничивающий фактор - необходимость агрегации результатов через `MPI_Allreduce`
   - Разница между `pipeline` и `task_run` режимами незначительна (7-9%), что соответствует ожиданиям для данной реализации

**Экспериментальные наблюдения:**
- Алгоритм **не демонстрирует масштабируемости** для данной задачи и размера данных
- Коммуникационные затраты (`MPI_Allreduce`) полностью нивелируют выигрыш от параллельной обработки строк
- Разница между `pipeline` и `task_run` режимами минимальна (7-9%), что соответствует ожиданиям для данной реализации
- Для задач с необходимостью полной агрегации результатов (как поиск глобальных минимумов) коммуникационные затраты становятся определяющим фактором

## 8. Заключение
Разработанный параллельный алгоритм нахождения минимальных значений по столбцам матрицы успешно прошел функциональные тесты, но не показал преимуществ в производительности по сравнению с последовательной реализацией. Для матрицы размером 5000×5000 MPI реализация оказалась в 2.5-3.1 раза медленнее SEQ реализации, демонстрируя ускорение 0.32-0.40 и эффективность 4-20%. Эксперименты выявили, что основной ограничивающий фактор - коммуникационные затраты, особенно операция `MPI_Allreduce`, требующая обмена данными между всеми процессами. Разница между pipeline и task_run режимами оказалась незначительной (1-9%), что соответствует теоретическим ожиданиям. Работа наглядно демонстрирует важность анализа соотношения вычислений и коммуникаций при проектировании параллельных алгоритмов.

## 9. Источники
1. Сысоев А. В. Курс лекций по параллельному программированию
2. Документация Open MPI     https://www.open-mpi.org/doc/
3. Microsoft Функции MPI     https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-functions