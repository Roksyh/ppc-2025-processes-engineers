# Ленточная горизонтальная схема - умножение матрицы на вектор
- **Студент**: Терехов Дмитрий Юрьевич 
- **группа:** 3823Б1ПР4
- **Технология**: SEQ | MPI
- **Вариант**: 11 (Ленточная горизонтальная схема - умножение матрицы на вектор)

## 1. Введение
Умножение матрицы на вектор — фундаментальная операция в линейной алгебре, широко используемая в научных вычислениях, компьютерной графике и машинном обучении. Для больших матриц последовательный алгоритм требует значительных вычислительных ресурсов, что делает актуальной задачу параллелизации.

 - Ожидаемый результат: создание корректного параллельного решения, демонстрирующего ускорение за счёт распределения вычислительной нагрузки между процессами с использованием ленточной горизонтальной схемы. 

## 2. Постановка задачи

**Цель работы:**
Реализовать и сравнить производительность последовательного (SEQ) и параллельного (MPI) алгоритмов умножения матрицы на вектор, основанного на ленточной горизонтальной схеме, где матрица распределяется по строкам между процессами, а вектор полностью рассылается всем процессам.

**Определение задачи:**
Для заданной матрицы A (размером M×K) и вектора B (размером K) необходимо вычислить вектор C = A × B (размером M), где каждый элемент вычисляется как:
C[i] = Σ (A[i][k] * B[k]) для k = 0..K-1

**Ограничения:**

- Входные данные - матрица и вектор совместимых размеров (столбцы A = размер B)
- Для параллельной реализации используется MPI с ленточной горизонтальной схемой
- Матрица A распределяется по строкам между процессами
- Вектор B полностью рассылается всем процессам
- Результат обеих реализаций (последовательной и параллельной) должен совпадать

## 3. Базовый алгоритм

### Алгоритм последовательной реализации

1. Инициализация
- Получить на вход матрицу matrix_A (M×K) и вектор vector_B (K)
- Инициализировать результирующий вектор result_vector размером M нулями
2. Умножение матрицы на вектор
- Для каждой строки i от 0 до M-1:
- Вычислить скалярное произведение: sum = 0
- Для каждого элемента k от 0 до K-1:
```cpp
sum = sum + matrix_A[i][k] * vector_B[k]
```
```cpp
result_vector[i] = sum
```
3. Возврат результата

- Вернуть вектор result_vector

**Код последовательной реализации:**

```cpp
bool TerekhovDHorizontalMatrixVectorSEQ::RunImpl() {
  const auto &matrix_a = input_.first;
  const auto &vector_b = input_.second;

  size_t rows_a = matrix_a.size();
  size_t cols_a = matrix_a[0].size();

  auto &output = GetOutput();
  output = std::vector<double>(rows_a, 0.0);

  for (size_t i = 0; i < rows_a; i++) {
    for (size_t j = 0; j < cols_a; j++) {
      output[i] += matrix_a[i][j] * vector_b[j];
    }
  }

  return true;
}
```
## 4. Схема распараллеливания
### 4.1. Распределение данных
**Для параллельной обработки используется ленточная горизонтальная схема распределения данных:**

- Матрица `A` распределяется по строкам между всеми процессами
- Вектор `B` полностью дублируется на всех процессах
- Каждый процесс работает только со своей частью матрицы A, но имеет полную копию вектора `B`

**Распределение строк матрицы A:**

- Общее количество строк: `rows_a`
- Количество процессов: `world_size_`
- Распределение выполняется по принципу: строка с индексом `i` обрабатывается процессом с рангом `i % world_size_ `
- Каждый процесс получает примерно равное количество строк

### 4.2. Схема связи и топология
**Используется звездообразная топология с процессом `0` в качестве центрального координатора:**

- Нисходящие связи (от процесса `0` к worker-процессам):
- Рассылка размеров матрицы и вектора `(MPI_Bcast)`
- Рассылка вектора B `(MPI_Bcast)`
- Распределение строк матрицы `A (MPI_Send)`
- Восходящие связи (от worker-процессов к процессу 0):
- Передача частичных результатов умножения `(MPI_Send)`

### 4.3. Ранжирование ролей
**Процесс 0 (Master-координатор):**

1. Инициализация данных

2. Распространение метаданных через `MPI_Bcast`

3. Распространение вектора `B` через `MPI_Bcast`

4. Распределение строк матрицы `A` через `MPI_Send`

5. Локальная обработка своих строк

6. Сбор результатов через `MPI_Recv`

7. Рассылка финального результата через `MPI_Bcast`

**Процессы 1..N-1 (Worker-процессы):**

1. Получение метаданных через `MPI_Bcast`

2. Получение вектора B через `MPI_Bcast`

3. Получение строк матрицы A через `MPI_Recv`

4. Локальная обработка полученных строк

5. Отправка результатов процессу `0` через `MPI_Send`

6. Получение финального результата через `MPI_Bcast`

### 4.4. Псевдокод
```cpp
function RunImpl():
    rank, size = MPI_comm_info()
    
    if size == 1:
        return RunSequential()
    
    // Получение метаданных
    if rank == 0:
        rows_a, cols_a, vector_size = get_sizes()
    
    MPI_Bcast([rows_a, cols_a, vector_size])
    
    // Проверка совместимости размеров
    if cols_a != vector_size:
        GetOutput() = empty_vector
        return false
    
    // Рассылка вектора B
    if rank == 0:
        vector_flat = flatten_vector(vector_B)
    
    MPI_Bcast(vector_flat)
    
    // Распределение строк матрицы A
    local_rows, my_row_indices = распределение_работы(rows_a, rank, size)
    
    if rank == 0:
        for dest от 1 до size-1:
            dest_rows = get_rows_for_process(dest, rows_a)
            send_rows_data(dest, dest_rows, cols_a)
        
        local_a_flat = get_my_rows_data(my_row_indices, cols_a)
    else:
        receive_rows_from_root(local_rows, my_row_indices, local_a_flat, cols_a)
    
    // Локальное умножение
    local_result = new double[local_rows]
    для i от 0 до local_rows-1:
        sum = 0
        для j от 0 до cols_a-1:
            sum += local_a_flat[i*cols_a + j] * vector_flat[j]
        local_result[i] = sum
    
    // Сбор и распределение результатов
    if rank == 0:
        final_result = new double[rows_a]
        собрать_свои_результаты(my_row_indices, local_result, final_result)
        
        для src от 1 до size-1:
            получить_результаты_от_процесса(src, final_result)
        
        MPI_Bcast(final_result)
    else:
        отправить_результаты(local_result, local_rows)
        получить_финальный_результат(final_result)
    
    GetOutput() = final_result
```

## 5. Детали реализации
### Структура кода
**Файловая структура:**
```cpp
Terekhov_D_Horizontal_matrix_vector/
├── common/include/common.hpp
├── seq/include/ops_seq.hpp
├── seq/src/ops_seq.cpp
├── mpi/include/ops_mpi.hpp
├── mpi/src/ops_mpi.cpp
├── tests/functional/main.cpp
├── tests/performance/main.cpp
└── settings.json
```
**Ключевые классы:**

**1. Последовательная реализация (seq):**

- `TerekhovDHorizontalMatrixVectorSEQ` - класс последовательной реализации

- Основной метод: `RunImpl()` - классический алгоритм умножения матрицы на вектор

**2. MPI реализация (`mpi`):**

- `TerekhovDHorizontalMatrixVectorMPI` - класс параллельной реализации

- **Основные методы:**

- - `RunImpl()` - основной алгоритм с распределением данных

- - `PrepareAndValidateSizes()` - рассылка размеров

- - `PrepareAndBroadcastVector()` - рассылка вектора

- - `DistributeMatrixAData()` - распределение матрицы

- - `ComputeLocalMultiplication()` - локальные вычисления

- - `GatherResults()` - сбор результатов

**Архитектурные особенности:**

- Использование MPI для межпроцессного взаимодействия
- Ленточная горизонтальная схема распределения данных
- Централизованная модель с процессом-координатором (rank 0)
- Минимизация коммуникационных операций
- Обработка граничных случаев (пустые матрицы, несовместимые размеры)

### Важные допущения и ограничения
**Обработка данных:**

- Поддержка матриц произвольных совместимых размеров
- Использование типа double для элементов матриц и векторов
- Проверка корректности размеров в `ValidationImpl()`

**Граничные случаи:**

- Пустая матрица или вектор
- Несовместимые размеры (столбцы матрицы ≠ размер вектора)
- Матрица 1×N или вектор 1×1
- Очень большие матрицы (обработка производительности)

## 6. Экспериментальная установка
### Аппаратное обеспечение и ОС
**Системные характеристики:**

- Модель процессора: intel core i5 10400f
- Архитектура: x86_64
- Ядра/потоки: 6 ядер 12 потоков
- Оперативная память: 32 GB 
- Операционная система: Windows 10
- Тип системы: PC (personal computer)

### Набор инструментов
**Компиляция и сборка:**

- Компиляторы: GCC и Clang
- Среда разработки: Visual Studio Code
- Стандарт языка: C++17
- Система сборки: CMake
- Тип сборки: Release

### Управление процессами
Запуск тестов:
```cpp
bash
mpirun --allow-run-as-root -n 4 ./build/bin/ppc_func_tests
mpirun --allow-run-as-root -n 4 ./build/bin/ppc_perf_tests
```

## 7. Результаты и обсуждение
### 7.1 Корректность
**Методы проверки корректности:**

1. Комплексное модульное тестирование:

-  - 22 функциональных теста - проверка базовых сценариев

-  - 10 тестов покрытия - обработка граничных случаев

2. Тестирование производительности:

- - Тест производительности с измерением времени выполнения

### Ключевые тестовые сценарии:

```cpp
// Базовые тесты умножения
[[1,2],[3,4]] × [5,6] = [17,39]
[[1,0],[0,1]] × [1,2] = [1,2]       // Единичная матрица
[[1,1],[1,1]] × [1,1] = [2,2]       // Матрица из единиц
[[0,0],[0,0]] × [1,2] = [0,0]       // Нулевая матрица

// Граничные случаи
[[1,2,3]] × [4,5,6] = [32]          // Вектор-строка
[[1],[2],[3]] × [4] = [4,8,12]      // Вектор-столбец
[[2]] × [3] = [6]                   // 1×1 матрица
```
### Результаты проверки корректности:

- Все 22 функциональных теста пройдены успешно

- Все 10 тестов покрытия пройдены успешно

- Результаты SEQ и MPI реализаций полностью совпадают

- Общее количество пройденных тестов: 161 тест

### 7.2 Производительность
**Результаты измерения производительности для матрицы 800×800:**

### Время выполнения (task_run) - чистые вычисления
| Режим | Процессы | Время, с | Ускорение | Эффективность |
|-------|----------|----------|-----------|---------------|
| seq   | 1        | 0.0007779   | 1.00      | 100%          |
| mpi   | 1        | 0.0007544   | 1.03     | 103%           |
| mpi   | 2        | 0.0016780   | 0.46     | 23%           |
| mpi   | 3        | 0.0014132  | 0.55     | 18%           |
| mpi   | 4        | 0.0011656  | 0.67      | 17%           |
### Время выполнения (pipeline) - полный цикл
| Режим | Процессы | Время, с | Ускорение | Эффективность |
|-------|----------|----------|-----------|---------------|
| seq   | 1        | 0.0007379   | 1.00      | 100%          |
| mpi   | 1        | 0.0007682   | 0.96     | 96%           |
| mpi   | 2        | 	0.0019357  | 0.38      | 19%           |
| mpi   | 3        | 0.0019918   | 	0.37     | 12%           |
| mpi   | 4        | 0.0012407   | 0.59     | 15%           |
### Сравнение SEQ и MPI при 1 процессе
| Метрика | SEQ  |  MPI(1 процесс) | Разница |
|-------|----------|----------|-----------|
| task_run   | 0.0007779       | 0.0007544   | +3.0%     |
| pipeline   | 0.0007379       | 0.0007682   | -4.1%      |

Особенность результатов:
MPI реализация с 1 процессом показывает сравнимую с SEQ версией производительность:

- Task_Run: MPI на 3% быстрее SEQ (0.0007544 с vs 0.0007779 с)

- Pipeline: MPI на 4% медленнее SEQ (0.0007682 с vs 0.0007379 с)

**Анализ результатов:**

**1. Основные наблюдения:**

- MPI версия с 1 процессом работает примерно так же быстро, как SEQ версия (разница в пределах 4%)

- При добавлении большего числа процессов MPI версия становится медленнее последовательной

- Наилучшее время достигается при 1 процессе для обеих версий

**2. Ускорение и эффективность:**

- Отрицательное ускорение: MPI версия с 2-4 процессами медленнее SEQ версии

- Низкая эффективность: от 12% до 23% для pipeline и task_run соответственно

- Пик производительности MPI: на 4 процессах для task_run (0.67× ускорение)

**3. Сравнение task_run и pipeline:**

- task_run показывает немного лучшие результаты, чем pipeline

- Разница обусловлена накладными расходами на инициализацию и финализацию в pipeline

- Для 4 процессов pipeline показывает более стабильное поведение

**Ключевые выводы:**

1. Для данных размеров (800×800) MPI версия не демонстрирует преимущества над последовательной реализацией

2. Накладные расходы на коммуникации превышают выигрыш от параллельных вычислений

3. Оптимальная конфигурация: 1 процесс (последовательная версия)

4. Алгоритм масштабируется отрицательно при увеличении числа процессов для данного размера данных

Обновленный раздел 8. Выводы:
## 8. Выводы
### Достижения
**1. Функциональная корректность:**

- Все 161 тест пройден успешно

- SEQ и MPI версии дают идентичные результаты

- Реализована полноценная ленточная горизонтальная схема

**2. Архитектурная корректность:**

- Правильное использование MPI API

- Корректное распределение данных

- Обработка граничных случаев

### Ограничения и проблемы
**1. Производительность для тестовых данных:**

- MPI версия с 2-4 процессами медленнее последовательной

- Максимальное ускорение: 0.67× (при 4 процессах для task_run)

- Эффективность: 12-23% от теоретического максимума

**2. Ограничения масштабируемости:**

- Алгоритм демонстрирует отрицательное масштабирование для данных размеров

## 9. Список литературы
 
1. Сысоев А. В. Лекции по параллельному программированию. — Н. Новгород: ННГУ, 2025.
2. Документация Open MPI     https://www.open-mpi.org/doc/
3. Microsoft Функции MPI     https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-functions